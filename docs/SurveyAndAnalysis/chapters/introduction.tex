\chapter{Introduction}
Machine learning has been an active area of research in recent years and has been deployed in a number of tasks. Machine learning models often use data from a single view, or modality, such as an array of sensors on a single device or the image stream of a video. However, recent efforts show that combining data from multiple modalities can increase accuracy [cite captioning, 2, other mm]. Multimodal learning comes with a number of challenges, including deciding when to fuse data from each modality and dealing with failure in individual modalities.\\

Existing research and implementations often assume that the multimodal data supplied to the model is complete and clean [cite]. However, in real world conditions data from a particular modality or sensor may be corrupted or completely missing, resulting in inaccurate results from such a model. Multimodal models should either understand when they have been fed corrupted samples, suggesting the output could be inaccurate, or be trained in such a way that they are robust to them.\\ 

This project aims to maximise model performance in the presence of missing and corrupt modalities. This report will present existing methods of anomaly and distribution shift detection, data reconstruction, and model robustness, as well as other technologies that could be adapted for these purposes. It will also explore possible next steps \\