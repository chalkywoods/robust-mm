\chapter{CCA anomaly detection}

We propose a multimodal anomaly detection system based on the correlations between inputs produced by DGCCA.

\section{Canonical Correlation Analysis}
\subsection{Linear CCA}
Canonical Correlation Analysis \cite{CCA} is a a generalised way of analysing cross covariance matrices between two random variables. Given two random variables $\mathbf{X}=(X_1,...,X_n | X_i\in\mathbb{R}^{d_X})$ and $\mathbf{Y}=(Y_1,...,Y_n | Y_i\in\mathbb{R}^{d_Y})$, CCA learns transformations $\mathbf{u}_1\in\mathbb{R}^{d_X}$ and $\mathbf{v}_1\in\mathbb{R}^{d_Y}$ such that $$corr(\mathbf{u}_1^T\mathbf{X}, \mathbf{v}_1^T\mathbf{Y}) \textrm{ is maximised.}$$ $(\mathbf{u}_1^T\mathbf{X}, \mathbf{v}_1^T\mathbf{Y})$ are known as the first pair of canonical variables. Further canonical variables maximise correlation orthogonally to the first pair of canonical variables, with the $i^{th}$ pair of canonical variables $(\mathbf{u}_i^T\mathbf{X}, \mathbf{v}_i^T\mathbf{Y})$ maximising $$corr(\mathbf{u}_i^T\mathbf{X}, \mathbf{v}_i^T\mathbf{Y})$$ subject to $$corr(\mathbf{u}_i^T\mathbf{X}, \mathbf{u}_j^T\mathbf{X}) = corr(\mathbf{v}_i^T\mathbf{Y}, \mathbf{v}_j^T\mathbf{Y}) = 0,\thinspace  \forall \thinspace  j<i$$

In practice, CCA is carried out by performing singular value decomposition on the correlation matrix. \\

The transformations learned by CCA give the basis for our anomaly detection system. When two modalities contain clean data of the type encountered during training, we expect their canonical variables to have a high correlation. If a modality contains corrupted data which differs enough from the training distribution, the transformation will not be optimal and we expect the correlation to be lower.\\

As discussed, CCA is limited to maximising correlations between two sets of variates using linear transformations. To use CCA efficiently with many modalities we require a method of learning transformations for more than two sets of variates.

\subsection{Generalised Canonical Correlation Analysis}
GCCA \cite{GCCA} constructs a shared representation $G$ and maximises the correlations between each set of variates and  the shared representation.

\subsection{Deep Generalised Canonical Correlation Analysis}
DGCCA builds upon Deep CCA \cite{DCCA} to extend GCCA to learn nonlinear transformations. A neural network is trained for each modality to learn a nonlinear transformation to a new representation which is used for GCCA. The networks are trained by backpropagating the objective of GCCA, maximising the ability of their outputs to be correlated.\\

Benton et al. \cite{DGCCA} derive a loss function for GCCA based on pairwise correlations between modalities.\\

Using DGCCA we can obtain representations of any number of modalities, with the expectation that correlation between any two representations is high.

\section{Corruption detection}
To observe the level of correlation between two modalities we require multiple samples. We use the transformations learned by DGCCA to generate a representation of each modality. To calculate the correlation between modalities $a$ and $b$ based on $N$ samples with cca dimension $C$, we have canonical variate matrices $A, B \in \mathbb{R}^{N\times C}$ and calculate the combined correlation, the sum of correlations over each canonical variate:

$$CombinedCorr(A, B) = \sum_{d=1}^C{corr((A_{s,d} | s=1..N), (B_{s,d} | s=1..N))}$$

The expectation is that this measure is high when samples are drawn from the training distribution, but low when samples are corrupted and fall outside of it.\\

To detect whether a given pair of modalities is corrupted, we compare their correlation with a threshold learned during training. Combined correlation is calculated between cca embeddings of clean data, as well as between clean and corrupt data. The intensity of corruption used affects the distribution of corrupt correlations, and therefore the learned threshold. We assume the correlations follow a normal distribution, and choose the threshold as the intersection between pdfs of clean and corrupt modalities. For $m$ modalities, we learn $\frac{1}{2}m(m+1)$ thresholds.\\

\todo[inline]{Image of modality corruption from pairwise corruption}

The pairwise corruption detection stage yields an truth matrix $M \in \{true, false\}^{m\times m}$, where $M_{i,j}$ is true if both modalities are clean, and false if one or both modalities are corrupted. We expect that when modality $i$ is corrupted, all entries of the ith row and column are false. The remaining modalities are true for all entries except that of the corrupted modality. Notably, the proportion of corrupt pairs in the row of a corrupt modality is greater than the overall proportion of corrupted pairs, whilst the proportion of corrupt pairs in the row of a clean modality is lower than this value. By comparing each row against this value, we can detect up to $m-2$ corrupt modalities.\\

Modality representation, could be raw or modality specific embedding.\\

DGCCA, use loss function to train CCA heads per modality\\

Train linear cca on head embeddings\\

Use of cca to detect corruption, windows etc\\

new dataset, add corruption\\

threshold on distribution of clean and corrupt pairs\\

Modality classification from pairwise classification\\
